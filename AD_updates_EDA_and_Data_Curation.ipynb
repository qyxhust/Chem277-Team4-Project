{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a041c1-50d8-4013-be73-a4bfb2f068e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a25643-b61c-48b8-ad3b-e52765bb36d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the GNPC supplementary table 5\n",
    "file_path = 'gnpc_supp.xlsx'\n",
    "sheet_name = 'SuppTbl5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c584026-7d7b-49a0-a458-a428f1a5ff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (7289, 75)\n",
      "      SeqId    SomaId                                     TargetFullName  \\\n",
      "0  10000-28  SL019233                                 Beta-crystallin B2   \n",
      "1   10001-7  SL002564  RAF proto-oncogene serine/threonine-protein ki...   \n",
      "2  10003-15  SL019245                             Zinc finger protein 41   \n",
      "3  10006-25  SL019228                ETS domain-containing protein Elk-1   \n",
      "4  10008-43  SL019234              Guanylyl cyclase-activating protein 1   \n",
      "\n",
      "  Target UniProt EntrezGeneID EntrezGeneSymbol Organism  \\\n",
      "0  CRBB2  P43320         1415           CRYBB2    Human   \n",
      "1  c-Raf  P04049         5894             RAF1    Human   \n",
      "2  ZNF41  P51814         7592            ZNF41    Human   \n",
      "3   ELK1  P19419         2002             ELK1    Human   \n",
      "4  GUC1A  P43080         2978           GUCA1A    Human   \n",
      "\n",
      "   Avg_StdBeta_weighted_AD  Meta_p_weighted_AD  ...  FTD_StdBeta_I   FTD_p_I  \\\n",
      "0                 0.020411            0.105892  ...       0.006329  0.928649   \n",
      "1                -0.018095            0.863149  ...       0.017468  0.804333   \n",
      "2                 0.049706            0.000772  ...      -0.051204  0.467474   \n",
      "3                 0.028990            0.002776  ...      -0.008240  0.907607   \n",
      "4                -0.014837            0.151340  ...      -0.002621  0.970121   \n",
      "\n",
      "   FTD_StdBeta_N   FTD_p_N  FTD_StdBeta_Q   FTD_p_Q  StdBeta_ALS     p_ALS  \\\n",
      "0       0.053975  0.250799      -0.026774  0.490741    -0.075116  0.201204   \n",
      "1      -0.017854  0.708493       0.040186  0.295369    -0.052058  0.369734   \n",
      "2       0.022361  0.632494       0.000016  0.999663     0.015578  0.790375   \n",
      "3      -0.020170  0.672558       0.038028  0.325208     0.088028  0.129374   \n",
      "4      -0.048115  0.310151      -0.060230  0.118636     0.030428  0.603799   \n",
      "\n",
      "   p_ALS_fdr  p_ALS_Bonf  \n",
      "0   0.842753         1.0  \n",
      "1   0.941329         1.0  \n",
      "2   0.995112         1.0  \n",
      "3   0.782800         1.0  \n",
      "4   0.990152         1.0  \n",
      "\n",
      "[5 rows x 75 columns]\n",
      "Index(['SeqId', 'SomaId', 'TargetFullName', 'Target', 'UniProt',\n",
      "       'EntrezGeneID', 'EntrezGeneSymbol', 'Organism',\n",
      "       'Avg_StdBeta_weighted_AD', 'Meta_p_weighted_AD', 'Meta_pval_FDR_AD',\n",
      "       'Meta_pval_Bonf_AD', 'Sig_pos_AD', 'Sig_neg_AD', 'max_sites_AD',\n",
      "       'AD_StdBeta_A', 'AD_p_A', 'AD_StdBeta_C', 'AD_p_C', 'AD_StdBeta_D',\n",
      "       'AD_p_D', 'AD_StdBeta_E', 'AD_p_E', 'AD_StdBeta_F', 'AD_p_F',\n",
      "       'AD_StdBeta_I', 'AD_p_I', 'AD_StdBeta_J', 'AD_p_J', 'AD_StdBeta_L',\n",
      "       'AD_p_L', 'AD_StdBeta_G', 'AD_p_G', 'AD_StdBeta_R', 'AD_p_R',\n",
      "       'Avg_StdBeta_weighted_PD', 'Meta_p_weighted_PD', 'Meta_pval_FDR_PD',\n",
      "       'Meta_pval_Bonf_PD', 'Sig_pos_PD', 'Sig_neg_PD', 'max_sites_PD',\n",
      "       'PD_StdBeta_C', 'PD_p_C', 'PD_StdBeta_F', 'PD_p_F', 'PD_StdBeta_J',\n",
      "       'PD_p_J', 'PD_StdBeta_L', 'PD_p_L', 'PD_StdBeta_Q', 'PD_p_Q',\n",
      "       'PD_StdBeta_R', 'PD_p_R', 'PD_StdBeta_T', 'PD_p_T',\n",
      "       'Avg_StdBeta_weighted_FTD', 'Meta_p_weighted_FTD', 'Meta_pval_FDR_FTD',\n",
      "       'Meta_pval_Bonf_FTD', 'Sig_pos_FTD', 'Sig_neg_FTD', 'max_sites_FTD',\n",
      "       'FTD_StdBeta_C', 'FTD_p_C', 'FTD_StdBeta_I', 'FTD_p_I', 'FTD_StdBeta_N',\n",
      "       'FTD_p_N', 'FTD_StdBeta_Q', 'FTD_p_Q', 'StdBeta_ALS', 'p_ALS',\n",
      "       'p_ALS_fdr', 'p_ALS_Bonf'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Read the Excel file, skipping the first row (header=1)\n",
    "# AD removed try - no matching except statment\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name, header=1)\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff29710-c740-452c-8595-bef8a0c81136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame shape: (7289, 9)\n",
      "  GeneSymbol   AD_beta      AD_p   PD_beta      PD_p  FTD_beta     FTD_p  \\\n",
      "0     CRYBB2  0.020411  0.105892 -0.019541  0.002026  0.016847  0.126806   \n",
      "1       RAF1 -0.018095  0.863149  0.004977  0.189185  0.007533  0.393355   \n",
      "2      ZNF41  0.049706  0.000772  0.004067  0.466674  0.000385  0.385409   \n",
      "3       ELK1  0.028990  0.002776  0.010033  0.580357  0.000471  0.457739   \n",
      "4     GUCA1A -0.014837  0.151340 -0.006160  0.018256 -0.054077  0.095542   \n",
      "\n",
      "   ALS_beta     ALS_p  \n",
      "0 -0.075116  0.201204  \n",
      "1 -0.052058  0.369734  \n",
      "2  0.015578  0.790375  \n",
      "3  0.088028  0.129374  \n",
      "4  0.030428  0.603799  \n"
     ]
    }
   ],
   "source": [
    "# Define the required columns and their new names\n",
    "\n",
    "required_columns = {\n",
    "    'EntrezGeneSymbol': 'GeneSymbol',\n",
    "    'Avg_StdBeta_weighted_AD': 'AD_beta',\n",
    "    'Meta_p_weighted_AD': 'AD_p',\n",
    "    'Avg_StdBeta_weighted_PD': 'PD_beta',\n",
    "    'Meta_p_weighted_PD': 'PD_p',\n",
    "    'Avg_StdBeta_weighted_FTD': 'FTD_beta',\n",
    "    'Meta_p_weighted_FTD': 'FTD_p',\n",
    "    'StdBeta_ALS': 'ALS_beta', # This is the non-weighted one for ALS\n",
    "    'p_ALS': 'ALS_p'\n",
    "}\n",
    "\n",
    "missing_cols = [col for col in required_columns.keys() if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"Error: The following required columns are missing: {missing_cols}\")\n",
    "else:\n",
    "    # Select and rename the columns\n",
    "    features_df = df[list(required_columns.keys())].copy() \n",
    "    features_df.rename(columns=required_columns, inplace=True)\n",
    "    \n",
    "    print(\"New DataFrame shape:\", features_df.shape)\n",
    "    print(features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "587da07e-0495-4431-bf6f-c6791b450043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique gene symbols: 6386\n",
      "Total number of rows: 7289\n",
      "Number of rows after dropping duplicates: 6386\n",
      "\n",
      "Missing values per column before cleaning:\n",
      "AD_beta     0\n",
      "AD_p        0\n",
      "PD_beta     0\n",
      "PD_p        0\n",
      "FTD_beta    0\n",
      "FTD_p       0\n",
      "ALS_beta    0\n",
      "ALS_p       0\n",
      "dtype: int64\n",
      "\n",
      "Missing values per column after cleaning:\n",
      "AD_beta     0\n",
      "AD_p        0\n",
      "PD_beta     0\n",
      "PD_p        0\n",
      "FTD_beta    0\n",
      "FTD_p       0\n",
      "ALS_beta    0\n",
      "ALS_p       0\n",
      "dtype: int64\n",
      "\n",
      "DataFrame after cleaning:\n",
      "             AD_beta      AD_p   PD_beta      PD_p  FTD_beta     FTD_p  \\\n",
      "GeneSymbol                                                               \n",
      "CRYBB2      0.020411  0.105892 -0.019541  0.002026  0.016847  0.126806   \n",
      "RAF1       -0.018095  0.863149  0.004977  0.189185  0.007533  0.393355   \n",
      "ZNF41       0.049706  0.000772  0.004067  0.466674  0.000385  0.385409   \n",
      "ELK1        0.028990  0.002776  0.010033  0.580357  0.000471  0.457739   \n",
      "GUCA1A     -0.014837  0.151340 -0.006160  0.018256 -0.054077  0.095542   \n",
      "\n",
      "            ALS_beta     ALS_p  \n",
      "GeneSymbol                      \n",
      "CRYBB2     -0.075116  0.201204  \n",
      "RAF1       -0.052058  0.369734  \n",
      "ZNF41       0.015578  0.790375  \n",
      "ELK1        0.088028  0.129374  \n",
      "GUCA1A      0.030428  0.603799  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51313/1917690465.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  features_df['GeneSymbol'].fillna('UNKNOWN', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing\n",
    "\n",
    "features_df['GeneSymbol'].fillna('UNKNOWN', inplace=True)\n",
    "features_df['GeneSymbol'] = features_df['GeneSymbol'].astype(str)\n",
    "\n",
    "# Some gene symbols might be \"GENE1;GENE2\". We take the first one.\n",
    "features_df['GeneSymbol'] = features_df['GeneSymbol'].apply(lambda x: x.split(';')[0])\n",
    "\n",
    "# Check for duplicates \n",
    "print(f\"\\nNumber of unique gene symbols: {features_df['GeneSymbol'].nunique()}\")\n",
    "print(f\"Total number of rows: {len(features_df)}\")\n",
    "# If there are duplicates, we will keep the first occurrence\n",
    "features_df.drop_duplicates(subset='GeneSymbol', keep='first', inplace=True)\n",
    "print(f\"Number of rows after dropping duplicates: {len(features_df)}\")\n",
    "\n",
    "# Set GeneSymbol as the index\n",
    "features_df.set_index('GeneSymbol', inplace=True)\n",
    "\n",
    "# Handle missing values (NaNs) in the data columns\n",
    "print(\"\\nMissing values per column before cleaning:\")\n",
    "print(features_df.isnull().sum())\n",
    "\n",
    "# Fill NaN values with 0.\n",
    "features_df.fillna(0, inplace=True)\n",
    "\n",
    "print(\"\\nMissing values per column after cleaning:\")\n",
    "print(features_df.isnull().sum())\n",
    "\n",
    "print(\"\\nDataFrame after cleaning:\")\n",
    "print(features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eba25e6b-41d1-4eb7-81f5-32f3be07a903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after adding -log10(p) features:\n",
      "             AD_beta      AD_p   PD_beta      PD_p  FTD_beta     FTD_p  \\\n",
      "GeneSymbol                                                               \n",
      "CRYBB2      0.020411  0.105892 -0.019541  0.002026  0.016847  0.126806   \n",
      "RAF1       -0.018095  0.863149  0.004977  0.189185  0.007533  0.393355   \n",
      "ZNF41       0.049706  0.000772  0.004067  0.466674  0.000385  0.385409   \n",
      "ELK1        0.028990  0.002776  0.010033  0.580357  0.000471  0.457739   \n",
      "GUCA1A     -0.014837  0.151340 -0.006160  0.018256 -0.054077  0.095542   \n",
      "\n",
      "            ALS_beta     ALS_p   AD_logp   PD_logp  FTD_logp  ALS_logp  \n",
      "GeneSymbol                                                              \n",
      "CRYBB2     -0.075116  0.201204  0.975136  2.693367  0.896860  0.696364  \n",
      "RAF1       -0.052058  0.369734  0.063914  0.723114  0.405216  0.432111  \n",
      "ZNF41       0.015578  0.790375  3.112235  0.330987  0.414078  0.102167  \n",
      "ELK1        0.088028  0.129374  2.556576  0.236305  0.339382  0.888151  \n",
      "GUCA1A      0.030428  0.603799  0.820047  1.738602  1.019804  0.219107  \n",
      "\n",
      "Final, ordered feature DataFrame:\n",
      "             AD_beta   AD_logp   PD_beta   PD_logp  FTD_beta  FTD_logp  \\\n",
      "GeneSymbol                                                               \n",
      "CRYBB2      0.020411  0.975136 -0.019541  2.693367  0.016847  0.896860   \n",
      "RAF1       -0.018095  0.063914  0.004977  0.723114  0.007533  0.405216   \n",
      "ZNF41       0.049706  3.112235  0.004067  0.330987  0.000385  0.414078   \n",
      "ELK1        0.028990  2.556576  0.010033  0.236305  0.000471  0.339382   \n",
      "GUCA1A     -0.014837  0.820047 -0.006160  1.738602 -0.054077  1.019804   \n",
      "\n",
      "            ALS_beta  ALS_logp  \n",
      "GeneSymbol                      \n",
      "CRYBB2     -0.075116  0.696364  \n",
      "RAF1       -0.052058  0.432111  \n",
      "ZNF41       0.015578  0.102167  \n",
      "ELK1        0.088028  0.888151  \n",
      "GUCA1A      0.030428  0.219107  \n",
      "\n",
      "Created and saved the feature matrix to 'protein_features.csv'\n",
      "Final shape of the feature matrix: (6386, 8)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering \n",
    "\n",
    "# Define a small constant to add to p-values to avoid log(0)\n",
    "epsilon = 1e-300\n",
    "\n",
    "# Create the -log10(p-value) features\n",
    "p_value_cols = ['AD_p', 'PD_p', 'FTD_p', 'ALS_p']\n",
    "for col in p_value_cols:\n",
    "    # The new column name will be i.e, 'AD_logp'\n",
    "    new_col_name = col.replace('_p', '_logp')\n",
    "    features_df[new_col_name] = -np.log10(features_df[col] + epsilon)\n",
    "\n",
    "print(\"\\nDataFrame after adding -log10(p) features:\")\n",
    "print(features_df.head())\n",
    "\n",
    "\n",
    "# Re-ordering the final feature matrix\n",
    "final_feature_order = [\n",
    "    'AD_beta', 'AD_logp',\n",
    "    'PD_beta', 'PD_logp',\n",
    "    'FTD_beta', 'FTD_logp',\n",
    "    'ALS_beta', 'ALS_logp'\n",
    "]\n",
    "\n",
    "final_features_df = features_df[final_feature_order]\n",
    "\n",
    "print(\"\\nFinal, ordered feature DataFrame:\")\n",
    "print(final_features_df.head())\n",
    "\n",
    "\n",
    "#Save\n",
    "output_path = 'protein_features.csv'\n",
    "final_features_df.to_csv(output_path)\n",
    "\n",
    "print(f\"\\nCreated and saved the feature matrix to '{output_path}'\")\n",
    "print(f\"Final shape of the feature matrix: {final_features_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc3a3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE BY AD: adding function to calculate weights for loss function\n",
    "def get_loss_weights(df):\n",
    "    '''\n",
    "    Calculates weights for loss function terms based on inverse variance of beta values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas DataFrame that includes the columns \"AD_beta\", \"PD_beta\", \"FTD_beta\", \"ALS_beta\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ad_wt, pd_wt, ftd_wt, als_wt : weights for each component of the combined loss function\n",
    "    '''\n",
    "\n",
    "    betas = df[[\"AD_beta\", \"PD_beta\", \"FTD_beta\", \"ALS_beta\"]]\n",
    "\n",
    "    # get variance of beta values for each disease \n",
    "    variances_by_disease = betas.var()\n",
    "\n",
    "    # calculate inverse variance to use as weight\n",
    "    ad_wt = 1/variances_by_disease[\"AD_beta\"]\n",
    "    pd_wt = 1/variances_by_disease[\"PD_beta\"]\n",
    "    ftd_wt = 1/variances_by_disease[\"FTD_beta\"]\n",
    "    als_wt = 1/variances_by_disease[\"ALS_beta\"]\n",
    "\n",
    "    return ad_wt, pd_wt, ftd_wt, als_wt\n",
    "\n",
    "loss_weights = get_loss_weights(final_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa6d2d60-9cf2-4bd4-af7b-ca6d0e587b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a mapping dictionary with 6387 entries.\n",
      "Example mappings: [('P43320', 'CRYBB2'), ('P04049', 'RAF1'), ('P51814', 'ZNF41'), ('P19419', 'ELK1'), ('P43080', 'GUCA1A')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create UniProt to Gene Symbol Mapping \n",
    "\n",
    "# Load the Original GNPC Data again to get the mapping \n",
    "file_path = 'gnpc_supp.xlsx'\n",
    "sheet_name = 'SuppTbl5'\n",
    "\n",
    "# Use header=1 \n",
    "original_df = pd.read_excel(file_path, sheet_name=sheet_name, header=1)\n",
    "    \n",
    "\n",
    "# Select only the two columns we need for mapping\n",
    "# Drop rows where either identifier is missing\n",
    "mapping_df = original_df[['UniProt', 'EntrezGeneSymbol']].dropna()\n",
    "\n",
    "mapping_df['EntrezGeneSymbol'] = mapping_df['EntrezGeneSymbol'].astype(str).apply(lambda x: x.split(';')[0])\n",
    "\n",
    "# Create the dictionary: {UniProt_ID: GeneSymbol}\n",
    "mapping_df = mapping_df.drop_duplicates(subset='UniProt')\n",
    "uniprot_to_gene_map = dict(zip(mapping_df['UniProt'], mapping_df['EntrezGeneSymbol']))\n",
    "\n",
    "print(f\"Created a mapping dictionary with {len(uniprot_to_gene_map)} entries.\")\n",
    "print(\"Example mappings:\", list(uniprot_to_gene_map.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9605f3ed-9a7d-46cb-a816-b7b1d5f5c048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 6386 unique gene symbols to use as our node list.\n",
      "\n",
      "Loaded raw edges file with UniProt IDs. Shape: (78950, 2)\n",
      "\n",
      "Filtering translated edges to match the nodes in our feature matrix\n",
      "Kept 78682 out of 78682 edges.\n",
      "Removed 0 self-loops.\n",
      " Removed 39301 duplicate edges.\n",
      "Final unique edge count: 39381\n",
      "\n",
      "Saved the cleaned and translated edge list to 'protein_edges_clean.csv'\n"
     ]
    }
   ],
   "source": [
    "# Translate, Filter, and Clean Edges \n",
    "\n",
    "# Get the master list of our 6,386 valid gene symbols\n",
    "final_features_df = pd.read_csv('protein_features.csv', index_col='GeneSymbol')\n",
    "valid_gene_symbols = set(final_features_df.index)\n",
    "print(f\"\\nLoaded {len(valid_gene_symbols)} unique gene symbols to use as our node list.\")\n",
    "\n",
    "edges_file_path = 'ppi_edges_symbols.csv' # This file contains UniProt IDs\n",
    "\n",
    "\n",
    "# Load the edges file, which has a header row like \"protein1, protein2\"\n",
    "edges_df_uniprot = pd.read_csv(edges_file_path, header=0, names=['protein1_uniprot', 'protein2_uniprot'])\n",
    "\n",
    "print(f\"\\nLoaded raw edges file with UniProt IDs. Shape: {edges_df_uniprot.shape}\")\n",
    "\n",
    "# Translation \n",
    "# Map the UniProt IDs in both columns to Gene Symbols using our dictionary\n",
    "edges_df_uniprot['protein1'] = edges_df_uniprot['protein1_uniprot'].map(uniprot_to_gene_map)\n",
    "edges_df_uniprot['protein2'] = edges_df_uniprot['protein2_uniprot'].map(uniprot_to_gene_map)\n",
    "\n",
    "# Drop rows where a translation failed \n",
    "translated_edges_df = edges_df_uniprot.dropna(subset=['protein1', 'protein2'])\n",
    "\n",
    "#FILTERING (on the translated Gene Symbols) \n",
    "original_edge_count = len(translated_edges_df)\n",
    "\n",
    "print(\"\\nFiltering translated edges to match the nodes in our feature matrix\")\n",
    "filtered_edges_df = translated_edges_df[\n",
    "    translated_edges_df['protein1'].isin(valid_gene_symbols) & \n",
    "    translated_edges_df['protein2'].isin(valid_gene_symbols)\n",
    "].copy()\n",
    "\n",
    "print(f\"Kept {len(filtered_edges_df)} out of {original_edge_count} edges.\")\n",
    "\n",
    "# Cleaning\n",
    "pre_self_loop_count = len(filtered_edges_df)\n",
    "filtered_edges_df = filtered_edges_df[filtered_edges_df['protein1'] != filtered_edges_df['protein2']]\n",
    "print(f\"Removed {pre_self_loop_count - len(filtered_edges_df)} self-loops.\")\n",
    "\n",
    "pre_dedupe_count = len(filtered_edges_df)\n",
    "sorted_edges = np.sort(filtered_edges_df[['protein1', 'protein2']].values, axis=1)\n",
    "unique_edges_df = pd.DataFrame(sorted_edges, columns=['protein1', 'protein2']).drop_duplicates()\n",
    "print(f\" Removed {pre_dedupe_count - len(unique_edges_df)} duplicate edges.\")\n",
    "print(f\"Final unique edge count: {len(unique_edges_df)}\")\n",
    "\n",
    "# Save \n",
    "clean_edges_output_path = 'protein_edges_clean.csv'\n",
    "unique_edges_df.to_csv(clean_edges_output_path, index=False, header=False)\n",
    "\n",
    "print(f\"\\nSaved the cleaned and translated edge list to '{clean_edges_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d5ab9-fe5e-4a01-838e-b8a0ca40d000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6386 node features.\n",
      "Loaded 39381 clean edges.\n",
      "\n",
      "Created mapping for 6386 genes to integer indices.\n",
      "Feature matrix X created with shape: torch.Size([6386, 8])\n",
      "Target matrix Y created with shape: torch.Size([6386, 4])\n",
      "Edge index tensor created with shape: torch.Size([2, 39381])\n",
      "\n",
      "Created train/validation/test masks.\n",
      "Training nodes: 4470\n",
      "Validation nodes: 958\n",
      "Test nodes: 958\n",
      "\n",
      "Assembled final PyG Data object:\n",
      "Data(x=[6386, 8], edge_index=[2, 39381], y=[6386, 4], train_mask=[6386], val_mask=[6386], test_mask=[6386], gene_symbols=[6386], loss_weights=[4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Create PyTorch Geometric Data Object \n",
    "\n",
    "# Load the node features we created\n",
    "features_df = pd.read_csv('protein_features.csv', index_col='GeneSymbol')\n",
    "\n",
    "# Load the clean edge list we just created\n",
    "edges_df = pd.read_csv('protein_edges_clean.csv', header=None, names=['protein1', 'protein2'])\n",
    "\n",
    "print(f\"Loaded {len(features_df)} node features.\")\n",
    "print(f\"Loaded {len(edges_df)} clean edges.\")\n",
    "\n",
    "# Prepare Data for PyTorch \n",
    "\n",
    "# 1. Create the protein-to-integer mapping\n",
    "# The order of proteins in features_df is our canonical order\n",
    "gene_symbols = features_df.index.tolist()\n",
    "gene_to_idx = {gene: i for i, gene in enumerate(gene_symbols)}\n",
    "print(f\"\\nCreated mapping for {len(gene_to_idx)} genes to integer indices.\")\n",
    "\n",
    "# 2. Create the feature tensor (X)\n",
    "# Convert the pandas DataFrame to a NumPy array, then to a PyTorch tensor\n",
    "X = torch.tensor(features_df.values, dtype=torch.float)\n",
    "print(f\"Feature matrix X created with shape: {X.shape}\")\n",
    "\n",
    "# 3. Create the target tensor (Y)\n",
    "# Our target is to predict the beta values\n",
    "y_df = features_df[['AD_beta', 'PD_beta', 'FTD_beta', 'ALS_beta']]\n",
    "Y = torch.tensor(y_df.values, dtype=torch.float)\n",
    "print(f\"Target matrix Y created with shape: {Y.shape}\")\n",
    "\n",
    "# 4. Create the edge_index tensor\n",
    "# Map the gene symbols in the edges_df to their integer indices\n",
    "edge_index = torch.tensor([\n",
    "    [gene_to_idx[p1] for p1 in edges_df['protein1']],\n",
    "    [gene_to_idx[p2] for p2 in edges_df['protein2']]\n",
    "], dtype=torch.long)\n",
    "print(f\"Edge index tensor created with shape: {edge_index.shape}\")\n",
    "\n",
    "# Create Node Masks for Splitting Data \n",
    "num_nodes = len(features_df)\n",
    "perm = torch.randperm(num_nodes) # Random permutation of indices\n",
    "\n",
    "# Splitting 70% for training, 15% for validation, 15% for testing\n",
    "train_end = int(0.7 * num_nodes)\n",
    "val_end = int(0.85 * num_nodes)\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "train_mask[perm[:train_end]] = True\n",
    "\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask[perm[train_end:val_end]] = True\n",
    "\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask[perm[val_end:]] = True\n",
    "\n",
    "print(\"\\nCreated train/validation/test masks.\")\n",
    "print(f\"Training nodes: {train_mask.sum().item()}\")\n",
    "print(f\"Validation nodes: {val_mask.sum().item()}\")\n",
    "print(f\"Test nodes: {test_mask.sum().item()}\")\n",
    "\n",
    "\n",
    "# Assemble the Final Data Object\n",
    "graph_data = Data(\n",
    "    x=X,\n",
    "    edge_index=edge_index,\n",
    "    y=Y,\n",
    "    train_mask=train_mask,\n",
    "    val_mask=val_mask,\n",
    "    test_mask=test_mask,\n",
    "    gene_symbols=gene_symbols, # Storing gene names for later interpretation\n",
    "    loss_weights = loss_weights # so we can access for weighted loss function\n",
    ")\n",
    "\n",
    "print(f\"\\nAssembled final PyG Data object:\\n{graph_data}\")\n",
    "\n",
    "\n",
    "# Save the Object to a File\n",
    "output_file = 'processed_graph.pt'\n",
    "torch.save(graph_data, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "277B_final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
