# Cross-Disease Proteome analysis using a Graph Attention Network

Our goal is to use a Graph Neural Network (GNN) to discover shared functional modules of plasma proteins across four neurodegenerative diseases (AD, PD, FTD, ALS) by integrating proteomics data with a protein-protein interaction (PPI) network.

## Project Workflow Summary

Our project is an end-to-end machine learning pipeline that proceeds in three main stages:

1.  **Data Curation:** We process raw proteomics data and a PPI network into a clean, model-ready graph format. This foundational work is prototyped in the `notebooks/` directory and finalized in `data/02-preprocessed/`.
2.  **Model Training:** We train a multi-task Graph Attention Network (GAT) to predict protein abundance changes across all four diseases. This is performed by the `entrypoint/train.py` script, which saves the best-performing model to the `models/` directory.
3.  **Analysis & Interpretation:** We load the trained model to extract its learned protein embeddings. We use these embeddings to perform dimensionality reduction (UMAP), quantitative cluster benchmarking (Silhouette Score), and visualization. This stage, run by `entrypoint/analyze.py`, produces the final plots and data files for biological interpretation.

## Repository Structure & File Guide

The repository is organized into the following directories. Here is a description of their purpose and key contents:

### `/data/`
This directory holds all project data, separated by processing stage.

*   `01-raw/`: Contains the original, unmodified source data.
    *   `gnpc_supp.xlsx` / `41591...xlsx`: The raw GNPC proteomics data from the *Nature Medicine* paper.
    *   `ppi_..._raw.csv` / `ppi_..._symbols.csv`: The raw protein-protein interaction data downloaded from the STRING database.
*   `02-preprocessed/`: Contains the clean, intermediate data files that serve as direct inputs for our model.
    *   `protein_features.csv`: A table of 6,386 unique proteins and their 8 engineered features (beta and -logp for 4 diseases).
    *   `protein_edges_clean.csv`: The final, high-confidence PPI network, containing 39,381 interactions between the proteins in our feature set.
    *   `processed_graph.pt`: The final, model-ready PyTorch Geometric `Data` object that bundles the features, edges, and data splits together.
*   `04-predictions/`: Contains the outputs generated by our analysis pipeline.
    *   `protein_clusters.csv`: A list mapping each protein's `GeneSymbol` to its algorithmically assigned cluster. This is the input for the final biological benchmarking step (e.g., using Metascape).

### `/entrypoint/`
Contains the primary, runnable Python scripts for executing the main stages of the project. **All scripts should be run from the project's root directory.**

*   `train.py`: The script for training our GAT model. It loads `processed_graph.pt`, runs the training loop with validation, and saves the best model to `/models/best_model.pt`.
*   `analyze.py`: The script for the main analysis. It loads the trained model, performs UMAP and HDBSCAN clustering, benchmarks the clustering quality with Silhouette Score, and generates the final plots and cluster assignment files.

### `/models/`
A directory for storing the trained model files.

*   `best_model.pt`: The saved state dictionary of the best-performing GAT model, ready to be loaded for analysis or inference.

### `/notebooks/`
Contains Jupyter notebooks used for exploration, prototyping, and step-by-step analysis.

*   `EDA_and_Data_Curation.ipynb`: The primary notebook showing the full data cleaning and preprocessing pipeline.
*   `GNPC_data_exploration.ipynb`: Initial exploratory data analysis of the GNPC dataset.
*   `Cluster_gene_annotations.ipynb`: A notebook used for analyzing the trial run (without the silhoutte scoring version, we will delete this later on.) `protein_clusters.csv` output and performing biological interpretation.

### `/plots/`
Contains all the figures and visualizations generated by the analysis scripts.

*   `protein_embeddings_clustered_umap.png`: The key result???a 2D visualization of the learned protein embeddings, colored by their assigned cluster.
*   *(You may also have a `cluster_profile_heatmap.png` here, which is the technical benchmark)*

### `/src/`
The core source code library for our project. It contains code that is imported by the entrypoint scripts.

*   `model.py`: Defines the `MultiTaskGNN` class, which is for our Graph Attention Network.
*   `pipelines/`: A directory intended for modular data processing pipelines. --to be updated--

### `requirements.txt`
A list of all Python packages required to run this project

## How to Run the Full Analysis

1.  **Set up the environment:**
    ```bash
    # Make sure you are in the project root directory
    conda create -n neuro_gnn python=3.11
    conda activate neuro_gnn
    pip install -r requirements.txt
    ```
2.  **Run the analysis script:**
    ```bash
    # The prefix is important on some systems to prevent hangs
    OMP_NUM_THREADS=1 python entrypoint/analyze.py
    ```
    This will use the pre-trained `models/best_model.pt` and generate the final plots and cluster data in the `/plots` and `/data/04-predictions` directories.
