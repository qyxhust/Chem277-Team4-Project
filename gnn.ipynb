{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e22ad5",
   "metadata": {},
   "source": [
    "3 Layer GCN\n",
    "layer 1: input size num_features, output size 1st hidden size (try 128), ReLU activation\n",
    "layer 2: input size 1st hidden size (try 128), output size 2nd hidden size (try 64), ReLU activation\n",
    "layer 3: input size 2nd hidden size (try 64), output size num_classes (try 64)\n",
    "layers 4, 5, 6, 7: linear layers for predictions (input size is 64, output size different for each layer)\n",
    "\n",
    "Why 128 and 64? We want first hidden size to be 4-8x that of num_features, and we want 2nd hidden size to be about half of 1st hidden size\n",
    "\n",
    "Model is being trained to predict:\n",
    "1) Avg_StdBeta_weighted_AD (protein assoc w disease progression/severity, for regression)\n",
    "2) Sig_pos_AD, Sig_neg_AD COMBINED into one binary column (both =1 or 0) (sig pos/neg assoc w disease status, for clasification)\n",
    "3) Combine Sig_pos_AD, Sig_neg_AD, Avg_StdBeta_weighted_AD to identify protein mechanistic role - driver, mediator, or bystander (classification)\n",
    "\n",
    "Start w AD for now, then expand to PD/FTD later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e57fb295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devle/miniconda3/envs/277B_final_project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import random\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.nn import Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7cad2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: create the PPI graph\n",
    "\n",
    "# load in nodes and edges\n",
    "nodes = pd.read_csv(\"nodes.csv\")\n",
    "edges = pd.read_csv(\"edges.csv\")\n",
    "\n",
    "# get edge indices as np array\n",
    "edge_indices_np = edges[['src', 'dst']].values\n",
    "\n",
    "# transpose array so it's in correct format for torch_geometric (2 x num_edges)\n",
    "edge_indices_transposed = edge_indices_np.transpose()\n",
    "\n",
    "# convert to pytorch tensor\n",
    "edge_index_tensor = torch.tensor(edge_indices_transposed, dtype = torch.long)\n",
    "\n",
    "# get edge weights \n",
    "edge_weights = torch.tensor(edges['weight'].values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba30337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize graph using NetworkX (TO DO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b4882b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5505, 69)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in GNPC data\n",
    "data = pd.read_csv(\"Final_Project_Data.csv\", header=1)\n",
    "data_no_na = data.fillna(0) # convert NAs to 0\n",
    "uniprot_col = data_no_na.iloc[:, 4]\n",
    "numerical_data = data_no_na.iloc[:, 8:]\n",
    "numerical_and_uniprot = pd.concat([uniprot_col, numerical_data], axis=1)\n",
    "data_no_duplicates = numerical_and_uniprot.groupby('UniProt', as_index=False).mean() # calculate mean of all duplicates (same UniProt value)\n",
    "\n",
    "# merge nodes and data\n",
    "proteins = nodes[['index', 'UniProt']]\n",
    "data_nodes_merge = pd.merge(proteins, data_no_duplicates, on = 'UniProt', how = 'left')\n",
    "all_data = data_nodes_merge.sort_values(by='index').reset_index(drop=True) # sort by index\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc04efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create X and Y\n",
    "\n",
    "# select columns to be used for X and Y (make sure to scale!)\n",
    "AD_cols = [col for col in all_data.columns if col.endswith(\"_AD\") or col.startswith(\"AD\")]\n",
    "y_cols = [\"Avg_StdBeta_weighted_AD\", \"Sig_pos_AD\", \"Sig_neg_AD\"]\n",
    "X_cols = [col for col in AD_cols if col not in y_cols]\n",
    "X_np = all_data[X_cols].values # features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_np) # scale X_np\n",
    "\n",
    "x = torch.tensor(X_scaled, dtype=torch.float) # convert to tensor(dtype=torch.float)\n",
    "\n",
    "# Y1: turn Avg_StdBeta_weighted_AD into tensor\n",
    "beta_np = all_data[y_cols].iloc[:, 0].values # Avg_StdBeta as np array\n",
    "beta_re = beta_np.reshape(-1, 1) # reshape to 2D for scaling\n",
    "beta_scaled = scaler.fit_transform(beta_re) # scale beta (being used for regression)\n",
    "y1 = torch.tensor(beta_scaled, dtype=torch.float) # convert to tensor(dtype=torch.float)\n",
    "\n",
    "# Y2: combine Sig_pos_AD and Sig_neg_AD and turn into tensor (dtype=long)\n",
    "sig_cols = all_data[y_cols].iloc[:, 1:]\n",
    "all_data['combined_sig'] = ((all_data['Sig_pos_AD'] == 1) | (all_data['Sig_neg_AD'] == 1)).astype(int) # if either are 1, value in combined col is 1, else 0\n",
    "sig_np = all_data['combined_sig'].values\n",
    "y2 = torch.tensor(sig_np, dtype=torch.long) # convert to tensor(dtype=torch.long)\n",
    "\n",
    "# Y3: classify proteins as drivers, mediators, and bystanders\n",
    "    # driver: Sig_pos_AD/Sig_neg_AD = 1 and StdBeta > 1 (class 0)\n",
    "    # mediator: Sig_pos_AD/Sig_neg_AD = 0 and StdBeta > 1 (class 1)\n",
    "    # bystanders: StdBeta < 1 (class 2)\n",
    "\n",
    "all_data['beta_scaled'] = beta_scaled # add scaled avg std beta values to df\n",
    "all_data['high_beta'] = all_data['beta_scaled'].abs() > 1.0 # 1 is stdev cutoff for Avg_StdBeta bc mean is 0 for scaled data, so 1 is 1 stdev\n",
    "\n",
    "# define three classes (drivers, mediators, bystanders)\n",
    "class_defs = [\n",
    "    # class 0 (driver)\n",
    "    (all_data['combined_sig']) & (all_data['high_beta']),\n",
    "    \n",
    "    # class 1 (mediator)\n",
    "    (~all_data['combined_sig']) & (all_data['high_beta']),\n",
    "\n",
    "    # class 2 (bystander)\n",
    "    (~all_data['high_beta'])\n",
    "]\n",
    "\n",
    "possible_classes = [0,1,2]\n",
    "all_data['mech_role'] = np.select(class_defs, possible_classes) # add col to specify mechanistic role of each protein\n",
    "beta_sig_np = all_data['mech_role'].values\n",
    "y3 = torch.tensor(beta_sig_np, dtype=torch.long) # convert to tensor (dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de62db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate torch_geometric.data.data Data object\n",
    "\n",
    "# create training, validation, and testing masks\n",
    "N = all_data.shape[0]\n",
    "split_labels = np.random.choice([0, 1, 2], N, p = [0.6, 0.2, 0.2]) # 0 is train, 1 is val, 2 is test\n",
    "\n",
    "train_mask = torch.tensor(split_labels==0, dtype=torch.bool)\n",
    "val_mask = torch.tensor(split_labels==1, dtype=torch.bool)\n",
    "test_mask = torch.tensor(split_labels==2, dtype=torch.bool)\n",
    "\n",
    "my_data = Data(x=x, y1=y1, y2=y2, y3=y3, edge_index=edge_index_tensor, edge_weights=edge_weights, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f3a8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 0, Loss: nan\n",
      "Epoch: 10, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 10, Loss: nan\n",
      "Epoch: 20, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 20, Loss: nan\n",
      "Epoch: 30, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 30, Loss: nan\n",
      "Epoch: 40, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 40, Loss: nan\n",
      "Epoch: 50, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 50, Loss: nan\n",
      "Epoch: 60, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 60, Loss: nan\n",
      "Epoch: 70, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 70, Loss: nan\n",
      "Epoch: 80, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 80, Loss: nan\n",
      "Epoch: 90, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 90, Loss: nan\n",
      "Epoch: 100, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 100, Loss: nan\n",
      "Epoch: 110, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 110, Loss: nan\n",
      "Epoch: 120, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 120, Loss: nan\n",
      "Epoch: 130, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 130, Loss: nan\n",
      "Epoch: 140, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 140, Loss: nan\n",
      "Epoch: 150, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 150, Loss: nan\n",
      "Epoch: 160, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 160, Loss: nan\n",
      "Epoch: 170, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 170, Loss: nan\n",
      "Epoch: 180, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 180, Loss: nan\n",
      "Epoch: 190, MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.06768953068592058\n",
      "Epoch 190, Loss: nan\n",
      "Testing:\n",
      "MAE for task 1: nan, Acc for task 2: 0.0, Acc for task 3: 0.05615942028985507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(nan, nan, nan, nan)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 layer multi-task GCN: not yet working, not sure why. Single models will probably be easier\n",
    "\n",
    "class My_GCN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_node_features, n_neuron1, n_neuron2, n_classes1, n_classes2):\n",
    "        super(My_GCN, self).__init__()\n",
    "        \n",
    "        self.conv1 = GCNConv(n_node_features, n_neuron1)\n",
    "        self.conv2 = GCNConv(n_neuron1, n_neuron2)\n",
    "        self.conv3 = GCNConv(n_neuron2, n_neuron2)\n",
    "        self.task1_out = Linear(n_neuron2, 1) # regression, predicting AvgStdBeta\n",
    "        self.task2_out = Linear(n_neuron2, n_classes1) # binary classification\n",
    "        self.task3_out = Linear(n_neuron2, n_classes2) # multi classification\n",
    "        self.task4_out = Linear(n_neuron2, n_node_features) # so network can learn from topology\n",
    "\n",
    "    def forward(self, data):\n",
    "        # should we add dropouts?\n",
    "        \n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_weights\n",
    "\n",
    "        x1 = self.conv1(x, edge_index, edge_weight = edge_weight)\n",
    "        x2 = F.relu(x1)\n",
    "        x3 = F.dropout(x2, p=0.5, training=self.training)\n",
    "\n",
    "        x4 = self.conv2(x3, edge_index, edge_weight = edge_weight)\n",
    "        x5 = F.relu(x4)\n",
    "        x6 = F.dropout(x5, p=0.5, training=self.training)\n",
    "\n",
    "        x7 = self.conv3(x6, edge_index, edge_weight = edge_weight)\n",
    "        out1 = self.task1_out(x7)\n",
    "        out2 = self.task2_out(x7)\n",
    "        out3 = self.task3_out(x7)\n",
    "        out4 = self.task4_out(x7)\n",
    "        \n",
    "        return out1, out2, out3, out4\n",
    "    \n",
    "class FitModel():\n",
    "    \n",
    "    def __init__(self, my_model, learning_rate: float = 0.01):\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(my_model.parameters(), lr = learning_rate)\n",
    "        self.model = my_model\n",
    "\n",
    "    def get_accuracies(self, data, mask, out1, out2, out3):\n",
    "\n",
    "        # use MAE for task 1\n",
    "        MAE_task1 = F.l1_loss(out1[mask], data.y1[mask])\n",
    "\n",
    "        # use sigmoid for task 2 (binary classification)    \n",
    "        YProb_task2 = torch.sigmoid(out2[mask])\n",
    "        YPred_task2 = torch.round(YProb_task2) # all values greater than 0.5 get rounded to 1\n",
    "        YTrue_task2 = data.y2[mask].float().unsqueeze(1) # reshapes to [num_nodes, 1]\n",
    "        acc_task2 = (YPred_task2 == YTrue_task2).sum().item() / YTrue_task2.shape[0]\n",
    "\n",
    "        # use argmax for task 3 (multi classification)\n",
    "        YPred_task3 = out3[mask].argmax(dim=1)\n",
    "        YTrue_task3 = data.y3[mask]  \n",
    "        acc_task3 = (YPred_task3 == YTrue_task3).sum().item() / YTrue_task3.shape[0]\n",
    "\n",
    "        return MAE_task1, acc_task2, acc_task3\n",
    "        \n",
    "    def Run(self, data, N_epochs: int = 200):\n",
    "        \n",
    "        for n in range(N_epochs):\n",
    "            \n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            out1, out2, out3, out4 = self.model(data)\n",
    "            loss1 = F.mse_loss(out1[data.train_mask], data.y1[data.train_mask])\n",
    "            loss2 = F.cross_entropy(out2[data.train_mask], data.y2[data.train_mask]) # reshapes to [num_nodes, 2] matrix and converts to float\n",
    "            loss3 = F.cross_entropy(out3[data.train_mask], data.y3[data.train_mask])\n",
    "            loss4 = F.mse_loss(out4[data.train_mask], data.x[data.train_mask])\n",
    "\n",
    "            total_loss = loss1 + 1.5*loss2 + 3*loss3 + 0.05*loss4 # TO DO: figure out what weights should be (weight*loss) - which tasks should be weighted more?\n",
    "\n",
    "            total_loss.backward()\n",
    "            \n",
    "            self.optimizer.step()\n",
    "\n",
    "            if n % 10 == 0:\n",
    "                # validation accuracy\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_out1, val_out2, val_out3, val_out4 = self.model(data)\n",
    "                MAE1_val, acc2_val, acc3_val = self.get_accuracies(data, data.val_mask, val_out1, val_out2, val_out3)\n",
    "                print(f'Epoch: {n}, MAE for task 1: {MAE1_val}, Acc for task 2: {acc2_val}, Acc for task 3: {acc3_val}')\n",
    "                print(f\"Epoch {n}, Loss: {total_loss.item():.4f}\")\n",
    "        \n",
    "        # testing accuracy\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_out1, test_out2, test_out3, test_out4 = self.model(data)\n",
    "        MAE1_test, acc2_test, acc3_test = self.get_accuracies(data, data.test_mask, test_out1, test_out2, test_out3)\n",
    "        print(\"Testing:\")\n",
    "        print(f'MAE for task 1: {MAE1_test}, Acc for task 2: {acc2_test}, Acc for task 3: {acc3_test}')\n",
    "\n",
    "        return total_loss.item(), loss1.item(), loss2.item(), loss3.item() \n",
    "            \n",
    "my_model = My_GCN(n_node_features=24, n_neuron1=128, n_neuron2=64, n_classes1=2, n_classes2=3)\n",
    "My_Fit = FitModel(my_model, 0.0001)\n",
    "My_Fit.Run(my_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "277B_final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
