{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56f154e-a14f-4497-bdf4-2dc967da4b5e",
   "metadata": {},
   "source": [
    "## Overview of the workflow for the EDA and Graph\n",
    "\n",
    "### EDA of the meta-analytic analysis of the GNPC data, exploring the summary statistics and also down the line we are building the graph from the gathered PPI network information through STRING database \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e70360d-884c-4eac-a6bd-0e64b4183521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x133196f70>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf862d17",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95a25643-b61c-48b8-ad3b-e52765bb36d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the GNPC supplementary table 5 for the summary statistics \n",
    "file_path = '/Users/yasemindilarasucu/Desktop/Chem277-Team4-Project/data/01-raw/gnpc_supp.xlsx'\n",
    "sheet_name = 'SuppTbl5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c584026-7d7b-49a0-a458-a428f1a5ff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (7289, 75)\n",
      "      SeqId    SomaId                                     TargetFullName  \\\n",
      "0  10000-28  SL019233                                 Beta-crystallin B2   \n",
      "1   10001-7  SL002564  RAF proto-oncogene serine/threonine-protein ki...   \n",
      "2  10003-15  SL019245                             Zinc finger protein 41   \n",
      "3  10006-25  SL019228                ETS domain-containing protein Elk-1   \n",
      "4  10008-43  SL019234              Guanylyl cyclase-activating protein 1   \n",
      "\n",
      "  Target UniProt EntrezGeneID EntrezGeneSymbol Organism  \\\n",
      "0  CRBB2  P43320         1415           CRYBB2    Human   \n",
      "1  c-Raf  P04049         5894             RAF1    Human   \n",
      "2  ZNF41  P51814         7592            ZNF41    Human   \n",
      "3   ELK1  P19419         2002             ELK1    Human   \n",
      "4  GUC1A  P43080         2978           GUCA1A    Human   \n",
      "\n",
      "   Avg_StdBeta_weighted_AD  Meta_p_weighted_AD  ...  FTD_StdBeta_I   FTD_p_I  \\\n",
      "0                 0.020411            0.105892  ...       0.006329  0.928649   \n",
      "1                -0.018095            0.863149  ...       0.017468  0.804333   \n",
      "2                 0.049706            0.000772  ...      -0.051204  0.467474   \n",
      "3                 0.028990            0.002776  ...      -0.008240  0.907607   \n",
      "4                -0.014837            0.151340  ...      -0.002621  0.970121   \n",
      "\n",
      "   FTD_StdBeta_N   FTD_p_N  FTD_StdBeta_Q   FTD_p_Q  StdBeta_ALS     p_ALS  \\\n",
      "0       0.053975  0.250799      -0.026774  0.490741    -0.075116  0.201204   \n",
      "1      -0.017854  0.708493       0.040186  0.295369    -0.052058  0.369734   \n",
      "2       0.022361  0.632494       0.000016  0.999663     0.015578  0.790375   \n",
      "3      -0.020170  0.672558       0.038028  0.325208     0.088028  0.129374   \n",
      "4      -0.048115  0.310151      -0.060230  0.118636     0.030428  0.603799   \n",
      "\n",
      "   p_ALS_fdr  p_ALS_Bonf  \n",
      "0   0.842753         1.0  \n",
      "1   0.941329         1.0  \n",
      "2   0.995112         1.0  \n",
      "3   0.782800         1.0  \n",
      "4   0.990152         1.0  \n",
      "\n",
      "[5 rows x 75 columns]\n",
      "Index(['SeqId', 'SomaId', 'TargetFullName', 'Target', 'UniProt',\n",
      "       'EntrezGeneID', 'EntrezGeneSymbol', 'Organism',\n",
      "       'Avg_StdBeta_weighted_AD', 'Meta_p_weighted_AD', 'Meta_pval_FDR_AD',\n",
      "       'Meta_pval_Bonf_AD', 'Sig_pos_AD', 'Sig_neg_AD', 'max_sites_AD',\n",
      "       'AD_StdBeta_A', 'AD_p_A', 'AD_StdBeta_C', 'AD_p_C', 'AD_StdBeta_D',\n",
      "       'AD_p_D', 'AD_StdBeta_E', 'AD_p_E', 'AD_StdBeta_F', 'AD_p_F',\n",
      "       'AD_StdBeta_I', 'AD_p_I', 'AD_StdBeta_J', 'AD_p_J', 'AD_StdBeta_L',\n",
      "       'AD_p_L', 'AD_StdBeta_G', 'AD_p_G', 'AD_StdBeta_R', 'AD_p_R',\n",
      "       'Avg_StdBeta_weighted_PD', 'Meta_p_weighted_PD', 'Meta_pval_FDR_PD',\n",
      "       'Meta_pval_Bonf_PD', 'Sig_pos_PD', 'Sig_neg_PD', 'max_sites_PD',\n",
      "       'PD_StdBeta_C', 'PD_p_C', 'PD_StdBeta_F', 'PD_p_F', 'PD_StdBeta_J',\n",
      "       'PD_p_J', 'PD_StdBeta_L', 'PD_p_L', 'PD_StdBeta_Q', 'PD_p_Q',\n",
      "       'PD_StdBeta_R', 'PD_p_R', 'PD_StdBeta_T', 'PD_p_T',\n",
      "       'Avg_StdBeta_weighted_FTD', 'Meta_p_weighted_FTD', 'Meta_pval_FDR_FTD',\n",
      "       'Meta_pval_Bonf_FTD', 'Sig_pos_FTD', 'Sig_neg_FTD', 'max_sites_FTD',\n",
      "       'FTD_StdBeta_C', 'FTD_p_C', 'FTD_StdBeta_I', 'FTD_p_I', 'FTD_StdBeta_N',\n",
      "       'FTD_p_N', 'FTD_StdBeta_Q', 'FTD_p_Q', 'StdBeta_ALS', 'p_ALS',\n",
      "       'p_ALS_fdr', 'p_ALS_Bonf'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Read the Excel file, skipping the first row (header=1)\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name, header=1)\n",
    "    print(\"DataFrame shape:\", df.shape)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "except Exception as e:\n",
    "    print(\"Error reading Excel file:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ff29710-c740-452c-8595-bef8a0c81136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame shape: (7289, 9)\n",
      "  GeneSymbol   AD_beta      AD_p   PD_beta      PD_p  FTD_beta     FTD_p  \\\n",
      "0     CRYBB2  0.020411  0.105892 -0.019541  0.002026  0.016847  0.126806   \n",
      "1       RAF1 -0.018095  0.863149  0.004977  0.189185  0.007533  0.393355   \n",
      "2      ZNF41  0.049706  0.000772  0.004067  0.466674  0.000385  0.385409   \n",
      "3       ELK1  0.028990  0.002776  0.010033  0.580357  0.000471  0.457739   \n",
      "4     GUCA1A -0.014837  0.151340 -0.006160  0.018256 -0.054077  0.095542   \n",
      "\n",
      "   ALS_beta     ALS_p  \n",
      "0 -0.075116  0.201204  \n",
      "1 -0.052058  0.369734  \n",
      "2  0.015578  0.790375  \n",
      "3  0.088028  0.129374  \n",
      "4  0.030428  0.603799  \n"
     ]
    }
   ],
   "source": [
    "# Define the required columns and their new names\n",
    "\n",
    "required_columns = {\n",
    "    'EntrezGeneSymbol': 'GeneSymbol',\n",
    "    'Avg_StdBeta_weighted_AD': 'AD_beta',\n",
    "    'Meta_p_weighted_AD': 'AD_p',\n",
    "    'Avg_StdBeta_weighted_PD': 'PD_beta',\n",
    "    'Meta_p_weighted_PD': 'PD_p',\n",
    "    'Avg_StdBeta_weighted_FTD': 'FTD_beta',\n",
    "    'Meta_p_weighted_FTD': 'FTD_p',\n",
    "    'StdBeta_ALS': 'ALS_beta', # This is the non-weighted one for ALS\n",
    "    'p_ALS': 'ALS_p'\n",
    "}\n",
    "\n",
    "missing_cols = [col for col in required_columns.keys() if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"Error: The following required columns are missing: {missing_cols}\")\n",
    "else:\n",
    "    # Select and rename the columns\n",
    "    features_df = df[list(required_columns.keys())].copy() \n",
    "    features_df.rename(columns=required_columns, inplace=True)\n",
    "    \n",
    "    print(\"New DataFrame shape:\", features_df.shape)\n",
    "    print(features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c8c19f5-6bb0-475d-9378-b6b743481b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique gene symbols: 6386\n",
      "Total number of rows: 7289\n",
      "Number of rows after dropping duplicates: 6386\n",
      "\n",
      "Missing values per column before cleaning:\n",
      "AD_beta     0\n",
      "PD_beta     0\n",
      "FTD_beta    0\n",
      "ALS_beta    0\n",
      "AD_p        0\n",
      "PD_p        0\n",
      "FTD_p       0\n",
      "ALS_p       0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after cleaning:\n",
      "AD_beta     0\n",
      "PD_beta     0\n",
      "FTD_beta    0\n",
      "ALS_beta    0\n",
      "AD_p        0\n",
      "PD_p        0\n",
      "FTD_p       0\n",
      "ALS_p       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing\n",
    "\n",
    "features_df['GeneSymbol'].fillna('UNKNOWN', inplace=True)\n",
    "features_df['GeneSymbol'] = features_df['GeneSymbol'].astype(str)\n",
    "\n",
    "# Some gene symbols might be \"GENE1;GENE2\". We take the first one.\n",
    "features_df['GeneSymbol'] = features_df['GeneSymbol'].apply(lambda x: x.split(';')[0])\n",
    "\n",
    "# Check for duplicates \n",
    "print(f\"\\nNumber of unique gene symbols: {features_df['GeneSymbol'].nunique()}\")\n",
    "print(f\"Total number of rows: {len(features_df)}\")\n",
    "# If there are duplicates, we will keep the first occurrence\n",
    "features_df.drop_duplicates(subset='GeneSymbol', keep='first', inplace=True)\n",
    "print(f\"Number of rows after dropping duplicates: {len(features_df)}\")\n",
    "\n",
    "# Set GeneSymbol as the index\n",
    "features_df.set_index('GeneSymbol', inplace=True)\n",
    "\n",
    "beta_cols    = ['AD_beta', 'PD_beta', 'FTD_beta', 'ALS_beta']\n",
    "p_value_cols = ['AD_p', 'PD_p', 'FTD_p', 'ALS_p']\n",
    "\n",
    "print(\"\\nMissing values per column before cleaning:\")\n",
    "print(features_df[beta_cols + p_value_cols].isnull().sum())\n",
    "\n",
    "# Betas: missing → 0 (no effect)\n",
    "features_df[beta_cols] = features_df[beta_cols].fillna(0.0)\n",
    "\n",
    "# p-values: missing → 1 (non-significant)\n",
    "features_df[p_value_cols] = features_df[p_value_cols].fillna(1.0)\n",
    "\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(features_df[beta_cols + p_value_cols].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702b43b-6ccf-4757-a9c4-aac6513737be",
   "metadata": {},
   "source": [
    "## Feature Engineering \n",
    "\n",
    "We are building a feature matrix from the GNPC's summarys statistics\n",
    "\n",
    "- `*_beta` columns has the standardized effect size for each disease \n",
    "  (case vs control: AD, PD, FTD, ALS)\n",
    "  \n",
    "- `*_logp` columns has the transformed p-values, computed as `-log10(p + ε)` to have usable numbers \n",
    "\n",
    "\n",
    "We keep only these 8 summary-statistic features:\n",
    "\n",
    "- AD_beta, AD_logp  \n",
    "- PD_beta, PD_logp  \n",
    "- FTD_beta, FTD_logp  \n",
    "- ALS_beta, ALS_logp  \n",
    "\n",
    "These will be the basis of our graph model:\n",
    "\n",
    "- The **betas** serve as the supervised targets (what we try to predict later on)\n",
    "- The **–log10(p)** values are part of the input feature vector, encoding the strength  \n",
    "  of evidence that each protein is dysregulated in each disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eba25e6b-41d1-4eb7-81f5-32f3be07a903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after adding -log10(p) features:\n",
      "             AD_beta      AD_p   PD_beta      PD_p  FTD_beta     FTD_p  \\\n",
      "GeneSymbol                                                               \n",
      "CRYBB2      0.020411  0.105892 -0.019541  0.002026  0.016847  0.126806   \n",
      "RAF1       -0.018095  0.863149  0.004977  0.189185  0.007533  0.393355   \n",
      "ZNF41       0.049706  0.000772  0.004067  0.466674  0.000385  0.385409   \n",
      "ELK1        0.028990  0.002776  0.010033  0.580357  0.000471  0.457739   \n",
      "GUCA1A     -0.014837  0.151340 -0.006160  0.018256 -0.054077  0.095542   \n",
      "\n",
      "            ALS_beta     ALS_p   AD_logp   PD_logp  FTD_logp  ALS_logp  \n",
      "GeneSymbol                                                              \n",
      "CRYBB2     -0.075116  0.201204  0.975136  2.693367  0.896860  0.696364  \n",
      "RAF1       -0.052058  0.369734  0.063914  0.723114  0.405216  0.432111  \n",
      "ZNF41       0.015578  0.790375  3.112235  0.330987  0.414078  0.102167  \n",
      "ELK1        0.088028  0.129374  2.556576  0.236305  0.339382  0.888151  \n",
      "GUCA1A      0.030428  0.603799  0.820047  1.738602  1.019804  0.219107  \n",
      "\n",
      "Final, ordered feature DataFrame:\n",
      "             AD_beta   AD_logp   PD_beta   PD_logp  FTD_beta  FTD_logp  \\\n",
      "GeneSymbol                                                               \n",
      "CRYBB2      0.020411  0.975136 -0.019541  2.693367  0.016847  0.896860   \n",
      "RAF1       -0.018095  0.063914  0.004977  0.723114  0.007533  0.405216   \n",
      "ZNF41       0.049706  3.112235  0.004067  0.330987  0.000385  0.414078   \n",
      "ELK1        0.028990  2.556576  0.010033  0.236305  0.000471  0.339382   \n",
      "GUCA1A     -0.014837  0.820047 -0.006160  1.738602 -0.054077  1.019804   \n",
      "\n",
      "            ALS_beta  ALS_logp  \n",
      "GeneSymbol                      \n",
      "CRYBB2     -0.075116  0.696364  \n",
      "RAF1       -0.052058  0.432111  \n",
      "ZNF41       0.015578  0.102167  \n",
      "ELK1        0.088028  0.888151  \n",
      "GUCA1A      0.030428  0.219107  \n",
      "\n",
      "Created and saved the feature matrix to '/Users/yasemindilarasucu/Desktop/Chem277-Team4-Project/data/02-preprocessed/protein_features.csv'\n",
      "Final shape of the feature matrix: (6386, 8)\n"
     ]
    }
   ],
   "source": [
    "# Define a small constant to add to p-values to avoid log(0)\n",
    "epsilon = 1e-300\n",
    "\n",
    "# Creating the -log10(p-value) features from the raw p-value columns\n",
    "p_value_cols = ['AD_p', 'PD_p', 'FTD_p', 'ALS_p']\n",
    "for col in p_value_cols:\n",
    "    new_col_name = col.replace('_p', '_logp')  # example: AD_p -> AD_logp\n",
    "    features_df[new_col_name] = -np.log10(features_df[col] + epsilon)\n",
    "\n",
    "print(\"\\nDataFrame after adding -log10(p) features:\")\n",
    "print(features_df.head())\n",
    "\n",
    "# Defining now which columns should go into protein_features.csv\n",
    "# We keep only betas + logp, no raw p-values from the summarys statistics\n",
    "final_feature_order = [\n",
    "    'AD_beta',  'AD_logp',\n",
    "    'PD_beta',  'PD_logp',\n",
    "    'FTD_beta', 'FTD_logp',\n",
    "    'ALS_beta', 'ALS_logp',\n",
    "]\n",
    "\n",
    "# checking if all the columns exist \n",
    "missing = [c for c in final_feature_order if c not in features_df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns for protein_features.csv: {missing}\")\n",
    "\n",
    "final_features_df = features_df[final_feature_order]\n",
    "\n",
    "print(\"\\nFinal, ordered feature DataFrame:\")\n",
    "print(final_features_df.head())\n",
    "\n",
    "# Save\n",
    "\n",
    "output_dir = '/Users/yasemindilarasucu/Desktop/Chem277-Team4-Project/data/02-preprocessed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, 'protein_features.csv')\n",
    "final_features_df.to_csv(output_path)\n",
    "\n",
    "print(f\"\\nCreated and saved the feature matrix to '{output_path}'\")\n",
    "print(f\"Final shape of the feature matrix: {final_features_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa6d2d60-9cf2-4bd4-af7b-ca6d0e587b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original GNPC table shape: (7289, 75)\n",
      "Created a mapping dictionary with 6387 entries.\n",
      "Example mappings: [('P43320', 'CRYBB2'), ('P04049', 'RAF1'), ('P51814', 'ZNF41'), ('P19419', 'ELK1'), ('P43080', 'GUCA1A')]\n"
     ]
    }
   ],
   "source": [
    "# Create UniProt to Gene Symbol Mapping \n",
    "\n",
    "file_path = '/Users/yasemindilarasucu/Desktop/Chem277-Team4-Project/data/01-raw/gnpc_supp.xlsx'\n",
    "sheet_name = 'SuppTbl5'\n",
    "\n",
    "# Load the Original GNPC Data to get the mapping \n",
    "original_df = pd.read_excel(file_path, sheet_name=sheet_name, header=1)\n",
    "print(\"Original GNPC table shape:\", original_df.shape)\n",
    "\n",
    "# Select only the columns we need for mapping and drop missing values\n",
    "mapping_df = original_df[['UniProt', 'EntrezGeneSymbol']].dropna().copy()\n",
    "\n",
    "# If a gene has multiple symbols separated by ';', keep the first one\n",
    "mapping_df['EntrezGeneSymbol'] = (\n",
    "    mapping_df['EntrezGeneSymbol']\n",
    "    .astype(str)\n",
    "    .apply(lambda x: x.split(';')[0])\n",
    ")\n",
    "\n",
    "# Ensure each UniProt appears only once\n",
    "mapping_df = mapping_df.drop_duplicates(subset='UniProt')\n",
    "\n",
    "# Create the dictionary: {UniProt_ID: GeneSymbol}\n",
    "uniprot_to_gene_map = dict(zip(mapping_df['UniProt'], mapping_df['EntrezGeneSymbol']))\n",
    "\n",
    "print(f\"Created a mapping dictionary with {len(uniprot_to_gene_map)} entries.\")\n",
    "print(\"Example mappings:\", list(uniprot_to_gene_map.items())[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767cabea-c69b-4cfa-8abf-d4e686ae74c8",
   "metadata": {},
   "source": [
    "### GNPC Summary Statistics → Feature Matrix\n",
    "\n",
    "- We start from the GNPC meta-analysis table which contains **7,289 rows** and **75 columns** of summary statistics across four neurodegenerative diseases (AD, PD, FTD, ALS)\n",
    "\n",
    "- After:\n",
    "  - cleaning `EntrezGeneSymbol` (splitting on `;`, keeping the first symbol),\n",
    "  - dropping ambiguous/missing symbols, and using unique gene symbols per row\n",
    "\n",
    "  we end up with **6,386 unique genes** \n",
    "\n",
    "- For each gene, we keep:\n",
    "  - Effect sizes (`*_beta`): AD_beta, PD_beta, FTD_beta, ALS_beta  \n",
    "    → these are the **targets** our model tries to predict\n",
    "  - Significance features (`*_logp`): –log10(p) for AD, PD, FTD, ALS  \n",
    "    → these are the **input node features**\n",
    "\n",
    "Conceptually, each node now represents **“how strongly and significantly this protein is dysregulated in each disease.”**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04845e8e-b6cd-482b-bb7d-314a83de2401",
   "metadata": {},
   "source": [
    "### Incorprated confidence scores from STRING\n",
    "\n",
    "Adding the confidence score from the STRING for the protein-protein interactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "742b2a35-a411-4819-ab4f-e8998c69c1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes path: /Users/yasemindilarasucu/Desktop/Chem277-Team4-Project/data/01-raw/ppi_nodes_raw.csv\n",
      "Edges path: /Users/yasemindilarasucu/Desktop/Chem277-Team4-Project/data/01-raw/ppi_edges_raw.csv\n",
      "Nodes exists? True\n",
      "Edges exists? True\n",
      "Loaded nodes file: (5505, 3)\n",
      "Loaded edges file: (80102, 4)\n"
     ]
    }
   ],
   "source": [
    "base_dir = '/Users/yasemindilarasucu/Desktop/Chem277-Team4-Project/data/01-raw'\n",
    "\n",
    "nodes_file_path = os.path.join(base_dir, 'ppi_nodes_raw.csv')   # node_id,index,UniProt\n",
    "edges_file_path = os.path.join(base_dir, 'ppi_edges_raw.csv')   # src,dst,weight,combined_score\n",
    "\n",
    "print(\"Nodes path:\", nodes_file_path)\n",
    "print(\"Edges path:\", edges_file_path)\n",
    "print(\"Nodes exists?\", os.path.exists(nodes_file_path))\n",
    "print(\"Edges exists?\", os.path.exists(edges_file_path))\n",
    "\n",
    "nodes_raw = pd.read_csv(nodes_file_path)\n",
    "edges_raw = pd.read_csv(edges_file_path)\n",
    "\n",
    "print(\"Loaded nodes file:\", nodes_raw.shape)\n",
    "print(\"Loaded edges file:\", edges_raw.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e14eb18-98a9-40f5-96f3-fa5be442b234",
   "metadata": {},
   "source": [
    "### Mapping STRING from UniProt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa183e91-e000-4a19-a787-80d48c737a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After mapping indices to UniProt:\n",
      "   src   dst protein1_uniprot protein2_uniprot\n",
      "0    0   904           P84085           O75154\n",
      "1    0  1878           P84085           P18085\n",
      "2    0  4898           P84085           P84077\n",
      "3    0  1629           P84085           P53367\n",
      "4    0  5307           P84085           P53365\n",
      "\n",
      "After UniProt→GeneSymbol mapping: 79834 edges remain\n",
      "6386 unique gene symbols to use as our node list\n",
      "Kept 79834 out of 79834 edges.\n",
      "Applied weight >= 0.7: kept 79834 / 79834 edges.\n",
      "Removed 130 self-loops.\n",
      "Removed duplicates; final edge count: 39381\n",
      "\n",
      "Saved the cleaned, weighted edge list to '/Users/yasemindilarasucu/Desktop/Chem277-Team4-Project/data/02-preprocessed/protein_edges_clean_weighted.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_w/09t1lq652qd_803897fd_b7h0000gn/T/ipykernel_77452/3431976242.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dedup_edges_df['protein1_clean'] = dedup_edges_df['pair'].apply(lambda p: p[0])\n",
      "/var/folders/_w/09t1lq652qd_803897fd_b7h0000gn/T/ipykernel_77452/3431976242.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dedup_edges_df['protein2_clean'] = dedup_edges_df['pair'].apply(lambda p: p[1])\n"
     ]
    }
   ],
   "source": [
    "# Map STRING indices -> UniProt\n",
    "idx_to_uniprot = nodes_raw.set_index('index')['UniProt'].to_dict()\n",
    "\n",
    "edges_raw['protein1_uniprot'] = edges_raw['src'].map(idx_to_uniprot)\n",
    "edges_raw['protein2_uniprot'] = edges_raw['dst'].map(idx_to_uniprot)\n",
    "\n",
    "print(\"\\nAfter mapping indices to UniProt:\")\n",
    "print(edges_raw[['src', 'dst', 'protein1_uniprot', 'protein2_uniprot']].head())\n",
    "\n",
    "# Map UniProt -> GeneSymbol \n",
    "\n",
    "edges_raw['protein1'] = edges_raw['protein1_uniprot'].map(uniprot_to_gene_map)\n",
    "edges_raw['protein2'] = edges_raw['protein2_uniprot'].map(uniprot_to_gene_map)\n",
    "\n",
    "# Drop rows where translation failed\n",
    "translated_edges_df = edges_raw.dropna(subset=['protein1', 'protein2']).copy()\n",
    "print(f\"\\nAfter UniProt→GeneSymbol mapping: {len(translated_edges_df)} edges remain\")\n",
    "\n",
    "\n",
    "# Set of proteins that actually have GNPC features\n",
    "valid_gene_symbols = set(features_df.index)\n",
    "\n",
    "print(f\"{len(valid_gene_symbols)} unique gene symbols to use as our node list\")\n",
    "\n",
    "# Now filter edges\n",
    "filtered_edges_df = translated_edges_df[\n",
    "    translated_edges_df['protein1'].isin(valid_gene_symbols) &\n",
    "    translated_edges_df['protein2'].isin(valid_gene_symbols)\n",
    "].copy()\n",
    "\n",
    "print(f\"Kept {len(filtered_edges_df)} out of {len(translated_edges_df)} edges.\")\n",
    "\n",
    "# Filtering by min confidence score:\n",
    "\n",
    "min_weight = 0.7\n",
    "pre_conf_count = len(filtered_edges_df)\n",
    "filtered_edges_df = filtered_edges_df[filtered_edges_df['weight'] >= min_weight].copy()\n",
    "print(f\"Applied weight >= {min_weight}: kept {len(filtered_edges_df)} / {pre_conf_count} edges.\")\n",
    "\n",
    "# Remove self-loops\n",
    "pre_self_loop_count = len(filtered_edges_df)\n",
    "filtered_edges_df = filtered_edges_df[filtered_edges_df['protein1'] != filtered_edges_df['protein2']]\n",
    "print(f\"Removed {pre_self_loop_count - len(filtered_edges_df)} self-loops.\")\n",
    "\n",
    "# Deduplicate undirected edges, keeping the highest-weight interaction per pair\n",
    "filtered_edges_df['pair'] = filtered_edges_df.apply(\n",
    "    lambda r: tuple(sorted((r['protein1'], r['protein2']))),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "filtered_edges_df = filtered_edges_df.sort_values('weight', ascending=False)\n",
    "dedup_edges_df = filtered_edges_df.drop_duplicates(subset='pair', keep='first')\n",
    "\n",
    "dedup_edges_df['protein1_clean'] = dedup_edges_df['pair'].apply(lambda p: p[0])\n",
    "dedup_edges_df['protein2_clean'] = dedup_edges_df['pair'].apply(lambda p: p[1])\n",
    "\n",
    "edges_final = dedup_edges_df[['protein1_clean', 'protein2_clean', 'weight']].rename(\n",
    "    columns={'protein1_clean': 'protein1', 'protein2_clean': 'protein2'}\n",
    ")\n",
    "\n",
    "print(f\"Removed duplicates; final edge count: {len(edges_final)}\")\n",
    "\n",
    "#save\n",
    "edges_output_dir = '/Users/yasemindilarasucu/Desktop/Chem277-Team4-Project/data/02-preprocessed'\n",
    "os.makedirs(edges_output_dir, exist_ok=True)\n",
    "\n",
    "clean_edges_output_path = os.path.join(edges_output_dir, 'protein_edges_clean_weighted.csv')\n",
    "edges_final.to_csv(clean_edges_output_path, index=False)\n",
    "\n",
    "print(f\"\\nSaved the cleaned, weighted edge list to '{clean_edges_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c756230-9117-400e-a3ae-fe810a79a5d8",
   "metadata": {},
   "source": [
    "### Building the protein–protein interaction graph for PyTorch Geometric\n",
    "\n",
    "As next we are combining the GNPC summary statistics with the STRING PPI network into\n",
    "a single `torch_geometric.data.Data` object:\n",
    "\n",
    "- **Nodes:** proteins present in `protein_features.csv` (indexed by GeneSymbol)\n",
    "- **Node features (`x`):** 4-dimensional vector of –log10(p) values  \n",
    "  (`AD_logp, PD_logp, FTD_logp, ALS_logp`)\n",
    "- **Targets (`y`):** 4-dimensional vector of disease-specific effect sizes  \n",
    "  (`AD_beta, PD_beta, FTD_beta, ALS_beta`), used for multi-task regression\n",
    "- **Edges (`edge_index`):** undirected protein–protein interactions derived from STRING,  \n",
    "  filtered to proteins in our feature matrix, with self-loops and duplicates removed\n",
    "- **Masks:** random train/validation/test splits over nodes (70% / 15% / 15%)\n",
    "\n",
    "This `processed_graph.pt` file is the main input to our GAT/GCN models: it encodes  \n",
    "both per-protein GNPC summary statistics and the known interaction structure from STRING.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87e2c454-d7c6-4e4d-95b5-46954bb26106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building PyG graph object from features + weighted edges\n",
      "X shape (features): torch.Size([6386, 4])\n",
      "Y shape (labels):   torch.Size([6386, 4])\n",
      "Loaded 39381 weighted edges.\n",
      "Created mapping for 6386 genes to integer indices.\n",
      "Edges after checking the presence in gene_to_idx: 39381\n",
      "edge_index shape: torch.Size([2, 39381])\n",
      "edge_attr shape:  torch.Size([39381, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBuilding PyG graph object from features + weighted edges\")\n",
    "\n",
    "# 1. Features: split into X (inputs) and Y (labels)\n",
    "label_cols   = ['AD_beta', 'PD_beta', 'FTD_beta', 'ALS_beta']\n",
    "feature_cols = ['AD_logp', 'PD_logp', 'FTD_logp', 'ALS_logp']\n",
    "\n",
    "missing_label_cols = [c for c in label_cols if c not in features_df.columns]\n",
    "missing_feat_cols  = [c for c in feature_cols if c not in features_df.columns]\n",
    "if missing_label_cols or missing_feat_cols:\n",
    "    raise ValueError(\n",
    "        f\"Missing columns.\\n\"\n",
    "        f\"  label_cols missing: {missing_label_cols}\\n\"\n",
    "        f\"  feature_cols missing: {missing_feat_cols}\"\n",
    "    )\n",
    "\n",
    "X = torch.tensor(features_df[feature_cols].values, dtype=torch.float32)\n",
    "Y = torch.tensor(features_df[label_cols].values,   dtype=torch.float32)\n",
    "\n",
    "print(\"X shape (features):\", X.shape)  # [num_nodes, 4]\n",
    "print(\"Y shape (labels):  \", Y.shape)  # [num_nodes, 4]\n",
    "\n",
    "# 2. Load weighted edge list\n",
    "edges_df = pd.read_csv('/Users/yasemindilarasucu/Desktop/Chem277-Team4-Project/data/02-preprocessed/protein_edges_clean_weighted.csv')  # protein1, protein2, weight\n",
    "print(f\"Loaded {len(edges_df)} weighted edges.\")\n",
    "\n",
    "# 3. Create mapping from gene symbol to node index\n",
    "gene_symbols = features_df.index.tolist()\n",
    "gene_to_idx = {gene: i for i, gene in enumerate(gene_symbols)}\n",
    "print(f\"Created mapping for {len(gene_to_idx)} genes to integer indices.\")\n",
    "\n",
    "# Ensure all edges refer to known genes \n",
    "edges_df = edges_df[\n",
    "    edges_df['protein1'].isin(gene_to_idx) &\n",
    "    edges_df['protein2'].isin(gene_to_idx)\n",
    "].copy()\n",
    "print(f\"Edges after checking the presence in gene_to_idx: {len(edges_df)}\")\n",
    "\n",
    "# 4. Build edge_index and edge_attr (confidence weights)\n",
    "edge_index = torch.tensor([\n",
    "    [gene_to_idx[p] for p in edges_df['protein1']],\n",
    "    [gene_to_idx[p] for p in edges_df['protein2']],\n",
    "], dtype=torch.long)\n",
    "\n",
    "edge_weight = torch.tensor(edges_df['weight'].values, dtype=torch.float32)\n",
    "edge_attr = edge_weight.view(-1, 1)  # shape [num_edges, 1]\n",
    "\n",
    "print(\"edge_index shape:\", edge_index.shape)\n",
    "print(\"edge_attr shape: \", edge_attr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee59619-4b72-4c2c-8b23-cf05df4ecb56",
   "metadata": {},
   "source": [
    "### STRING PPI Network → Cleaned and added the Weighted Edges\n",
    "\n",
    "- We first loaded the raw STRING data:\n",
    "  - Nodes: **5,505** entries (`node_id`, `index`, `UniProt`)\n",
    "  - Edges: **80,102** raw interactions with columns (`src`, `dst`, `weight`, `combined_score`).\n",
    "- Using the STRING node table, we map each edge’s integer indices (`src`, `dst`) to UniProt IDs, then use our GNPC-derived mapping to translate UniProt → GeneSymbol\n",
    "\n",
    "After this:\n",
    "\n",
    "- **79,834** edges can be mapped to *pairs* of valid gene symbols\n",
    "- All of these edges connect proteins that also appear in the GNPC feature matrix (good intersection between GNPC and STRING)\n",
    "\n",
    "We then:\n",
    "\n",
    "1. Filter to high-confidence interactions using the STRING `weight` (already in [0, 1]):\n",
    "   - Here, `weight ≥ 0.7` keeps **all** of the mapped edges, indicating that the supplied network is already high-confidence\n",
    "2. Remove self-loops (A–A interactions), removing **130** such edges\n",
    "3. Treat the graph as *undirected*:\n",
    "   - Canonicalize each edge to `(min(protein1, protein2), max(...))`\n",
    "   - Sort by weight and keep the highest-confidence edge per undirected pair\n",
    "\n",
    "This yields **39,381** unique undirected, high-confidence edges, which we save as:\n",
    "\n",
    "- `data/02-preprocessed/protein_edges_clean_weighted.csv`  \n",
    "  (`protein1`, `protein2`, `weight`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986d1aa8-4bab-490f-90bf-e1b33e34567b",
   "metadata": {},
   "source": [
    "## Spliting nodes and assemble the PyTorch Geometric `Data` object\n",
    "\n",
    "We randomly permute all proteins (nodes) and define:\n",
    "\n",
    "- **70%** of nodes as the training set  \n",
    "- **15%** as the validation set  \n",
    "- **15%** as the test set  \n",
    "\n",
    "For each split we create a boolean mask (`train_mask`, `val_mask`, `test_mask`) with:\n",
    "\n",
    "- `True` for nodes belonging to that split  \n",
    "- `False` otherwise  \n",
    "\n",
    "These masks are used during training and evaluation so that:\n",
    "\n",
    "- The model learns only from training nodes  \n",
    "- Hyperparameters are tuned on validation nodes  \n",
    "- Final performance is reported on held-out test nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d59885fa-9920-4006-855d-5b6d6c0ce0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training nodes:   4470\n",
      "Validation nodes: 958\n",
      "Test nodes:       958\n"
     ]
    }
   ],
   "source": [
    "# 5. Create train/val/test masks\n",
    "num_nodes = X.shape[0]\n",
    "perm = torch.randperm(num_nodes)\n",
    "\n",
    "train_end = int(0.7 * num_nodes)\n",
    "val_end   = int(0.85 * num_nodes)\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask   = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask  = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[perm[:train_end]]      = True\n",
    "val_mask[perm[train_end:val_end]] = True\n",
    "test_mask[perm[val_end:]]         = True\n",
    "\n",
    "print(f\"Training nodes:   {train_mask.sum().item()}\")\n",
    "print(f\"Validation nodes: {val_mask.sum().item()}\")\n",
    "print(f\"Test nodes:       {test_mask.sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675898a1-5f84-4db9-bf9c-75f3c398bba5",
   "metadata": {},
   "source": [
    "### We combine everything into a single PyTorch Geometric `Data` object:\n",
    "\n",
    "- `x` → node feature matrix `X` (log p-values for AD/PD/FTD/ALS)  \n",
    "- `y` → target matrix `Y` (effect sizes for AD/PD/FTD/ALS)  \n",
    "- `edge_index` → graph connectivity (who interacts with whom)  \n",
    "- `edge_attr` → edge confidence weights from STRING (1D edge features)  \n",
    "- `train_mask`, `val_mask`, `test_mask` → node splits for training and evaluation  \n",
    "- `gene_symbols` → list of gene names for later interpretation of embeddings and clusters  \n",
    "\n",
    "Finally, we save this graph as `data/02-preprocessed/processed_graph.pt`, which is the\n",
    "single input object used by all downstream GNN training and clustering scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3dc0cd56-1bfa-420e-8f32-f91edfe41ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assembled final PyG Data object:\n",
      "Data(x=[6386, 4], edge_index=[2, 39381], edge_attr=[39381, 1], y=[6386, 4], train_mask=[6386], val_mask=[6386], test_mask=[6386], gene_symbols=[6386])\n",
      "\n",
      "Saved graph data to '/Users/yasemindilarasucu/Desktop/Chem277-Team4-Project/data/02-preprocessed/processed_graph.pt'\n"
     ]
    }
   ],
   "source": [
    "# 6. Assemble final PyG Data object\n",
    "graph_data = Data(\n",
    "    x=X,\n",
    "    edge_index=edge_index,\n",
    "    edge_attr=edge_attr,      \n",
    "    y=Y,\n",
    "    train_mask=train_mask,\n",
    "    val_mask=val_mask,\n",
    "    test_mask=test_mask,\n",
    "    gene_symbols=gene_symbols,\n",
    ")\n",
    "\n",
    "print(\"\\nAssembled final PyG Data object:\")\n",
    "print(graph_data)\n",
    "\n",
    "# 7. Save to the path used by your training script\n",
    "output_file = '/Users/yasemindilarasucu/Desktop/Chem277-Team4-Project/data/02-preprocessed/processed_graph.pt'\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "torch.save(graph_data, output_file)\n",
    "\n",
    "print(f\"\\nSaved graph data to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ebdce-9077-46bd-853c-044a345c101e",
   "metadata": {},
   "source": [
    "### Final PyTorch Geometric Graph Object\n",
    "\n",
    "Using the cleaned features and edges, we construct a PyG `Data` object:\n",
    "\n",
    "- **Nodes:** 6,386 proteins (one per gene symbol)\n",
    "- **Node features (`x`):** shape `[6386, 4]`\n",
    "  - Columns: `[AD_logp, PD_logp, FTD_logp, ALS_logp]`\n",
    "- **Targets (`y`):** shape `[6386, 4]`\n",
    "  - Columns: `[AD_beta, PD_beta, FTD_beta, ALS_beta]`\n",
    "- **Edges (`edge_index`):** shape `[2, 39,381]`\n",
    "  - Each column is a pair of node indices (protein–protein interaction).\n",
    "- **Edge attributes (`edge_attr`):** shape `[39,381, 1]`\n",
    "  - Single scalar per edge: STRING confidence `weight` in [0, 1].\n",
    "- **Masks:**\n",
    "  - Training nodes: 4,470  \n",
    "  - Validation nodes: 958  \n",
    "  - Test nodes: 958  \n",
    "\n",
    "We saved this graph as:\n",
    "\n",
    "- `data/02-preprocessed/processed_graph.pt`\n",
    "\n",
    "This graph is exactly the object used by the downstream Multi-Task GNN (GATv2) model\n",
    "\n",
    "## What this preprocessing step prepares us to do\n",
    "\n",
    "At the end of this analysis in the notebook we have:\n",
    "\n",
    "- A **feature matrix** for 6,386 proteins:\n",
    "  - Inputs (node features): `AD_logp`, `PD_logp`, `FTD_logp`, `ALS_logp`\n",
    "    - how statistically strong the dysregulation is in each disease\n",
    "  - Targets (labels): `AD_beta`, `PD_beta`, `FTD_beta`, `ALS_beta`\n",
    "    - the direction and magnitude of the effect size per disease\n",
    "\n",
    "- A **STRING-based protein–protein interaction graph**:\n",
    "  - Nodes: the same 6,386 proteins that have GNPC summary statistics\n",
    "  - Edges: 39,381 high-confidence STRING interactions\n",
    "  - Edge attributes: STRING confidence scores in \\[0, 1]\n",
    "\n",
    "- A **PyTorch Geometric graph object**:\n",
    "  - `x`: node features (4D logp vector per protein)\n",
    "  - `y`: multi-task regression targets (4D beta vector per protein)\n",
    "  - `edge_index` + `edge_attr`: PPI structure and confidence\n",
    "  - train/val/test masks over nodes\n",
    "\n",
    "--> How this connects to the main project question ?\n",
    "\n",
    "Our main interest is **not** to “predict” effect sizes that we already know, but to use a graph neural network to learn network-aware embeddings of proteins that reflect:\n",
    "\n",
    "1. How each protein is dysregulated across the four neurodegenerative diseases, and  \n",
    "2. How it is positioned in the protein–protein interaction network\n",
    "\n",
    "We use a GAT-based multi-task regression model as a **representation learner**:\n",
    "\n",
    "- The loss (predicting `y` from `x`) forces the model to encode disease-specific dysregulation signals\n",
    "- The graph structure (STRING PPIs) lets each protein update its representation based on its neighbors, using attention to prioritize the most informative interactions\n",
    "\n",
    "After training, we extract the **learned protein embeddings** from the last GAT layer and:\n",
    "\n",
    "- Use UMAP + HDBSCAN to cluster proteins into **modules**\n",
    "- Characterize each module in terms of:\n",
    "  - average effect sizes across diseases (AD/PD/FTD/ALS),\n",
    "  - dominant disease patterns (AD-biased, PD-biased, mixed, pan-NDD),\n",
    "  - and we can do pathway enrichment analysis for the interesting clusters we identified, this can be a future direction since the timing limit, but a high-level overview can be still great\n",
    "\n",
    "--> this preprocessing pipeline sets up a graph where we can ask:\n",
    "\n",
    "--> “Given known PPI relationships and GNPC multi-disease summary stats, what protein modules emerge as co-dysregulated across neurodegenerative diseases?”*\n",
    "\n",
    "\n",
    "--> What we are adding with our model on top of the GNPC's already listed dysregulated proteins is: \n",
    "\n",
    "With the identified clusters in the embedding space, which gives us:\n",
    "\n",
    "Not “a list of 6,386 dysregulated proteins” but **a map of the protein space where nearby proteins share both**:\n",
    "similar multi-disease patterns and interaction context.\n",
    "We’re basically organizing the GNPC signals into a network of modules that might be useful and can give insights to the diagnostics or therapuetics \n",
    "\n",
    "**Our Goal:** Identify network-defined plasma protein modules that are co-dysregulated across AD, PD, FTD, and ALS, using GNPC summary statistics + STRING PPIs\n",
    "\n",
    "**What those modules represent:**\n",
    "Patterns of co-dysregulation in the blood, not pure biological pathways in brain\n",
    "\n",
    "**Likely mix of:**\n",
    "systemic inflammation, vascular changes, metabolic changes, maybe some brain-derived proteins leaking out.. Eventually we are looking at the plasma which a blood sample, and proteins aren't expressed in the blood, so these protein's are coming from somewhere else, from nearby organs tissues..\n",
    "\n",
    "With our project we are discovering plasma protein modules that show shared vs distinct patterns across NDDs, organized by protein–protein interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef80fd-8ee3-4e1a-bea0-e3224428b758",
   "metadata": {},
   "source": [
    "## At the end of this processing step, we have these files as an output:\n",
    "\n",
    "**protein_features.csv** = clean per-protein GNPC summary stats (features + labels)\n",
    "\n",
    "**protein_edges_clean_weighted.csv** = clean STRING PPI edges between those proteins\n",
    "\n",
    "**processed_graph.pt** = the PyG graph built from those two, plus splits, ready for training/clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
